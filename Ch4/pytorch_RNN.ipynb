{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3a4d47",
   "metadata": {},
   "source": [
    "## 1. Cha RNN(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14ba586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b533bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', 'a', 'e', 'l', 'p']\n",
      "size of character set : 5\n"
     ]
    }
   ],
   "source": [
    "# input : apple, output : pple!\n",
    "# this is simple example for understanding RNN\n",
    "# make character set\n",
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print(char_vocab)\n",
    "print ('size of character set : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d046be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "input_size = vocab_size # input size = character set size\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe540691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "# give a character a unique integer index\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) \n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1189928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "# make index_to_char to get results\n",
    "index_to_char={}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5ec25c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# map each character in the input data and label data to an integer\n",
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cba3f9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "# add batch dimension bacause nn.RNN() basically takes a 3D tensor as input.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f37d6401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "#one_hot_vector\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab5a6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change input data and label data to tensor\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df643ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : torch.Size([1, 5, 5])\n",
      "label data size : torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print('training data size : {}'.format(X.shape))\n",
    "print('label data size : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1425468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RNN model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d1c5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af71e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape) # 3D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a41141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, input_size).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3375bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9674d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1a6becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.6046030521392822 prediction:  [[3 3 3 3 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  llllp\n",
      "1 loss:  1.3746802806854248 prediction:  [[4 4 4 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppep\n",
      "2 loss:  1.1756865978240967 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "3 loss:  0.9675639867782593 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "4 loss:  0.8329417109489441 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "5 loss:  0.6868112683296204 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "6 loss:  0.5739648938179016 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "7 loss:  0.4184468686580658 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.29439520835876465 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.22024472057819366 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.1588030755519867 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.11608034372329712 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.08381578326225281 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.060750462114810944 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.04560647904872894 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.035663411021232605 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.028859863057732582 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.02399316430091858 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.020372191444039345 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.017581643536686897 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.015362024307250977 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.013547892682254314 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.012032344937324524 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.01074580755084753 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.009642122313380241 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.00868947058916092 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.00786514300853014 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.007151440717279911 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.006533878855407238 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0059999581426382065 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.005538743920624256 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.005140251480042934 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.004795714281499386 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.004497210495173931 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.004237778950482607 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.0040114084258675575 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.003812778042629361 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.0036374530754983425 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.003481648862361908 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.003342308569699526 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0032166745513677597 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.003102673217654228 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.002998535055667162 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0029029143042862415 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.002814630512148142 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0027327844873070717 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.002656643744558096 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.002585640177130699 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0025192538741976023 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0024571053218096495 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.002398911863565445 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.002344198524951935 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.002292918972671032 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.002244670642539859 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.002199264243245125 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.0021565579809248447 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0021162438206374645 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.002078179968520999 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.00204220088198781 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0020079745445400476 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.001975549617782235 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0019446642836555839 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0019151775632053614 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.001886994345113635 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.0018599249888211489 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.001833922229707241 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0018087964272126555 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0017845958936959505 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0017611298244446516 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0017384467646479607 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0017163800075650215 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.0016949772834777832 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.001674119965173304 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.001653855899348855 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.0016341134905815125 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0016149168368428946 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0015962186735123396 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0015779234236106277 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.001560126431286335 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0015427088364958763 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.001525790081359446 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0015092266257852316 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.001493042684160173 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0014772387221455574 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.001461719162762165 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.0014465793501585722 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0014316764427348971 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.001417034538462758 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0014026772696524858 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0013885570224374533 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.001374674029648304 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.0013610045425593853 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0013475484447553754 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.001334305852651596 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.0013213243801146746 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.001308532664552331 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.0012959546875208616 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0012836141977459192 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.0012713681207969785 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.0012593833962455392 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "#learning\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    result = outputs.data.numpy().argmax(axis=2)\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec8a46",
   "metadata": {},
   "source": [
    "## 1. Cha RNN(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "34810cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7968a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0b1d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = list(set(sentence))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d467a4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 0, 'o': 1, 'c': 2, 'h': 3, 'd': 4, '.': 5, \"'\": 6, 'm': 7, ' ': 8, 'i': 9, 'u': 10, ',': 11, 'y': 12, 'w': 13, 'g': 14, 'p': 15, 'k': 16, 'a': 17, 'l': 18, 'n': 19, 'b': 20, 'f': 21, 'r': 22, 't': 23, 'e': 24}\n"
     ]
    }
   ],
   "source": [
    "print(char_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5280c9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character set size : 25\n"
     ]
    }
   ],
   "source": [
    "dic_size = len(char_dic)\n",
    "print('character set size : {}'.format(dic_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da921c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b19196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9ee7daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 21, 8, 12, 1, 10, 8, 13, 17, 19]\n",
      "[21, 8, 12, 1, 10, 8, 13, 17, 19, 23]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[0])\n",
    "print(y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e0d39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f27298bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : torch.Size([170, 10, 25])\n",
      "label size : torch.Size([170, 10])\n"
     ]
    }
   ],
   "source": [
    "print('training data size : {}'.format(X.shape))\n",
    "print('label size : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc15c57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ee96666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21,  8, 12,  1, 10,  8, 13, 17, 19, 23])\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2544cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3f511b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dic_size, hidden_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "539e2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f951912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6b7e442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1700, 25])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.view(-1, dic_size).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "263efeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10])\n",
      "torch.Size([1700])\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f09c2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daaa,y,adad,aa,d,,aa,d,,,aadaa,,ya,a,,,d,,d,,daa,dda,,daadaa,,y,ya,ya,daad,a,d,a,,ya,d,dda,,daadadd,d,,d,a,,a,daaa,,a,daa,dad,da,ddaadad,,y,,d,aa,daaad,daa,ddaa,ydd,daaad,aa,ad,dd\n",
      "edppep,ebibaebebepb pebeebbpppedbapeppbyeb,ebepbep,bebeeb phpeyeybep,,ebeb,ebb,pedbapebip pbeebebebbpb,ebi,eepb,pbep,ebiebephbebebeebepebebedbaebeuheb b p,beb,ebebebebpep baeb bbe\n",
      "ettttthretrthetrttrrtrhrthrttrhttttrshthrttrrtersttrerhtt ttntthttrttthtethttttrthtttrrethtttrrthtrrthttrthrhth trttthttertttreterhtrnhtrtrethrttetrertetttrhthrhtrhrehhttrhttrthtn\n",
      " uflo ky tseo eo t et  e  t t et teo t t  t et e  oee  eo toee  t et eo ttet  e  tt t ettett et toes  o t to t  t et  t ee t eet o eo toee  t ee  t eo tole  t et ttt  eotet et e l\n",
      " o lo dolttdddet ttedlddoltdtdtt tteddddtddldtletltleoleddddldoleettldeldddedltlddtdtltdtdddldtldtdetltddtdtdll tldt lt ddt dldtltddeldtlttltddeldt ddddtletltddtltdddedtddtodtlddl\n",
      "   a  t eo to e  o  t to e t e o oo tt t t ot  o t e  eao t tt  o  o eo t t   o et t o tot t eo e tt eo t e  t eo  o e  et tt t eo ao t  t    to to t  t et e  a tot   to eo eo t e\n",
      "   to t  d te  d tmst   o m oee   eo tm o  ttm tu    u tm t  to eu to eo   o  to    eudt    o d  o t  tot  oom  tut   t sts ooed tued to to t  e  toem ed eu   eu tu ao e  t    eus\n",
      "tu to to d  ee t td tes tus ao  t to te   to dsos a  the stao t edst       td to  t tes au  oe  to   etodsto    to  tot ems to deth d t o      ta toe   d ts  ao  as to    toe   ts\n",
      "ls oe toed toplt tp ted tnd tod thtod  ao tem tonthe them tanlt t ht tonltoed tod thted tstthe  tond ttmd ton thtod tod tp t cemekhem tonlent aon toem m otm  tep tnctoem caoem   o\n",
      "t toeltoed tonlo to todhtoh tonct wouh ph t p eo thelt eo toelo th t aonltoed won t wed to theo toec taed ton t wo hthtset toecd theo tonlo t woe theo td ths aed w ttontheoheo t e\n",
      "thtae toed tonlo lo tn heth tonlt wo t lu uoud o thnlt eo toelo lo t ao lrond wonlt wns tn thet toekd tos wonlt wo lahtset toeld theo tonlo t woe theo cd wh taod r tth thetheo tt \n",
      "t ton to m tonlo ld tns'erh ton't aod' ah oend osto et eo to lo lo t aonl a t ton't ans t  thet to ks tos to  t au  rhthet to ct them to lo ' aor'theo  d ans aos ts th o etheo  t \n",
      "t,ton'tonm tonlo ld tn,hems tos't ao,' ah oeos o th  t er to to ep t aon  a t ton't ans t  theo to m  tos to  t aot d theo to k, them to to ' aor theo os d s tos ts th os theo  t \n",
      "t,ter tonk tonlu ld tnd im, dos'i aou' au oeo la to et em to to le t ao  eant don t ans im them to k  aod to  t aut a them to tt them to go t aor them od d s dos os th on dhem    \n",
      "t,te  tond to lu ld tndhim, donlt aouo lp teo le to et em to to le t do l ant dor t ans im them to k  and do  t autla them to th toem to to t aor them od e d dn, t  dh oe them    \n",
      "t,te  tond tonlu ld tnsher, donlt dout lh ten ee th et er to to le t to l and don t dns gs them to k  dnd tonkt eutla them to khetoem to to t for themend e s ios ts th oe theme i \n",
      "thtor tand tonbudld tnsher, don't aouo up tenpee th ether to lo ee t tonu and don't ans gs ther to k  dnd wonk, eut aother to khethem to lo t for therend ens iosets th of thereoic\n",
      "thtor tand tonbudld tsshir, won't aoup up penple th ether to lo le t tonu and won't ans gs them tonks and work, aut ayther to nhethem to bong for themend ens dosets th of themeoic\n",
      "thtor land wo budld tsshir, won't aoum up pfn le to lther th lo le t wonu and won't ass gs them tonks and wonk, aut ayther tornh them to bong for themend ans wns ns gh pf themeoic\n",
      "thtou land to butld t shik, don't aoup up pfnple to lther th lo le t woou and don't ass gm them tonksiand dork, aut ayther toanh them to le g for themend ens i s nsigh pf themeos'\n",
      "thteu land to butld t shik, don't aoup up penple to cther to lo le t dooc and don't ass gm them tonks and dook, aut aather teach them to leng for themeodlens ime nsity of themeos'\n",
      "g,teu uand to butld dsship, ton't aoup up penple togcther te lo le t dooc and don't ass gm them tonks and dook, aut aather teach them to leng for thereodlens immeosith of themeos'\n",
      "gytau uand to butld asship, ton't aoum bp people to cther to lo le t wood and don't assign them tosks and dook, aut aather toach them to leng for themeodless immeosity of themeos'\n",
      "gytau wand to build a ship, don't aoup up people to cther to lo lect wood and don't assign them tosks and dook, but aather teach them to leng for themeodless immensity of themeosc\n",
      "tytau wand to build a ship, don't aoum up people to ether te co lect wood and won't ansign them tasks and wook, but aather teach them ta long for themendless immensity of themeosc\n",
      "tytau tand to build a ship, don't doum up pe ple to ether to co lect wond and don't dssign them tasks and wook, but aather teach them ta leng for themendless immensity of theme sc\n",
      "pytau wand to build a ship, don't doum up people to ether to co lect word and don't dnsign them tasks and dork, but aather toach them ta long for the endless immensity of theme pc\n",
      "pytou wand to build a ship, don't doum up peoplestogether to co lect word and don't dssign them tasks and dork, but aather toach them ta long for themendless immensity of theme ac\n",
      "p tou wand to cuild a ship, don't drum up people together to collect word and don't dssign them tasks and dork, but rather teach them ta long for themendless immensity of theme ac\n",
      "p tou wand to build a ship, don't drum up people together to lollect word and don't dssign them tasks and dork, but rather teach them ta long for the endless immensity of the eeac\n",
      "p wou wand to build a ship, don't drum up people together to collect word and don't dssign them tasks and dork, but rather toach them ta long for the endless immensity of the eeac\n",
      "p wou wand to build a ship, don't drum up people together to collect word and don't dssign them tasks and dork, but rather teach them ta long for the endless immensity of the eeac\n",
      "p uou wand to build a ship, don't arum up people together to collect word and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the eeac\n",
      "l uou want to build a ship, don't arum up people together to collect word and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the eeac\n",
      "l uou want to build a ship, don't drum up people together to collect word and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eea'\n",
      "l uou want to build a ship, don't drum up people together to collect word and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eeac\n",
      "l wou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the eeas\n",
      "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the eea.\n",
      "p wou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the eea.\n",
      "p uou want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "l uou want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "g uou want to build a ship, don't arum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "g uou want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "g uou want to build a ship, don't drum up people together to collect wood and won't assign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "g uou want to build a ship, don't drum up people together to collect wood and won't dssign them tasks and work, but rather teach them ta long for the endless immensity of the eea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and won't dssign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and won't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the ees.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0:\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else:\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ccc54",
   "metadata": {},
   "source": [
    "## 2. Word RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9eeb1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a46d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Repeat is the best medicine for memory\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9191bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'medicine', 'memory', 'for', 'best', 'the', 'Repeat']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(sentence))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd5210bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}\n",
    "word2index['<unk>']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0dcba2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'medicine': 2, 'memory': 3, 'for': 4, 'best': 5, 'the': 6, 'Repeat': 7, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5af6d3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(word2index['memory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02635c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'is', 2: 'medicine', 3: 'memory', 4: 'for', 5: 'best', 6: 'the', 7: 'Repeat', 0: '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "index2word = {v: k for k, v in word2index.items()}\n",
    "print(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a5798dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medicine\n"
     ]
    }
   ],
   "source": [
    "print(index2word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d049b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(sentence, word2index):\n",
    "    encoded = [word2index[token] for token in sentence]\n",
    "    input_seq, label_seq = encoded[:-1], encoded[1:]\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0)\n",
    "    label_seq = torch.LongTensor(label_seq).unsqueeze(0)\n",
    "    return input_seq, label_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1e3f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = build_data(sentence, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1781931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 1, 6, 5, 2, 4]])\n",
      "tensor([[1, 6, 5, 2, 4, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c73dc0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=input_size)\n",
    "        self.rnn_layer = nn.RNN(input_size, hidden_size,\n",
    "                                batch_first=batch_first)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.embedding_layer(x)\n",
    "        output, hidden = self.rnn_layer(output)\n",
    "        output = self.linear(output)\n",
    "        return output.view(-1, output.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97160312",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2index)\n",
    "input_size = 5\n",
    "hidden_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11ded397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7d11e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0700, -0.3024,  0.0092, -0.0520,  0.2978, -0.2156,  0.0899,  0.0271],\n",
      "        [-0.0531, -0.1779,  0.0213, -0.3063,  0.0517,  0.0114, -0.2487,  0.3695],\n",
      "        [ 0.0617, -0.2082,  0.0437, -0.0776,  0.3013, -0.0668, -0.0947,  0.1383],\n",
      "        [-0.0770, -0.1891,  0.1045, -0.1660,  0.1797,  0.1060, -0.2883,  0.2868],\n",
      "        [-0.0204,  0.3934, -0.1830, -0.4981, -0.7915,  0.3647, -0.1341,  0.1486],\n",
      "        [-0.0738, -0.0184, -0.0733, -0.1485, -0.0581, -0.0052, -0.0504,  0.3315]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd7bdf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 8])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33e759a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode = lambda y: [index2word.get(x) for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2cae8412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/201] 2.3212 \n",
      "Repeat for Repeat for Repeat is Repeat\n",
      "\n",
      "[41/201] 1.7671 \n",
      "Repeat is medicine best medicine best memory\n",
      "\n",
      "[81/201] 1.0808 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[121/201] 0.5512 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[161/201] 0.2987 \n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[201/201] 0.1801 \n",
      "Repeat is the best medicine for memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = loss_function(output, Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 40 == 0:\n",
    "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
    "        pred = output.softmax(-1).argmax(-1).tolist()\n",
    "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f83083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
